{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*This notebook is intended to accompany the Cytiva UNICORN Tetra Data Workshop in TetraU. Specific references to Labels and sample data can be substituted for other ÄKTA data.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMS3k-zOI4St"
      },
      "source": [
        "# Configure Python Imports\n",
        "\n",
        "In order to run our Jupyter Notebook,  we will need the following libraries:\n",
        "- requests:  This library is used to make HTTP Requests to the TDP API\n",
        "- json:  Allows us to manipulate files as JSON Objects\n",
        "- pandas:  This is a very useful library for storing data in tabular structures\n",
        "- numpy:  Open Source Framework for mathematical computation\n",
        "- matplotlib:  Library for creating visualizations of your data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "frzYlhOhI-M_"
      },
      "outputs": [],
      "source": [
        "import requests, json, pandas, numpy, matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfoRFRwaJA4t"
      },
      "source": [
        "# Configure Connection Variables\n",
        "\n",
        "Create and store information on how to connect to the TDP API. Set the following values in the code cell below, inside the empty quotation marks:\n",
        "\n",
        "* Set `tdpHostname` to point at the correct URL for your TDP environment (such as \"platform.tetrascience.com\" without a \"https://\" or the forward slash at the end)\n",
        "* Set `orgSlug` to be your org slug (ex: `training-jsmith`)\n",
        "* Set `userToken` to an auth token, either your user's personal JWT or a Service User's token\n",
        " * To get your user's JWT value, open or return to a Chrome tab with TDP, click on the main hamburger menu in the top left corner and select My Account. You should be presented with the Account details screen. Click the \"Copy Token\" button. Your clipboard will now have the token needed for calling the API.\n",
        "* Set `userLabel` to match the value that you had entered in Lab 1 for your “user” Label. It should look similar to the following: [first_name]-[last_name]. E.g. if your name is John Smith, you would have used `john-smith`. Verify this value on a file or in the Agent configuration if you do not get results in later steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyv1pbBkJC4a"
      },
      "outputs": [],
      "source": [
        "# Example hostnames: \"platform.tetrascience.com\" \"tetrascience-uat.com\"\n",
        "tdpHostname = \"\"\n",
        "orgSlug = \"\"\n",
        "userToken = \"\"\n",
        "userLabel = \"\"\n",
        "\n",
        "headers = {\"x-org-slug\": orgSlug, \"ts-auth-token\": userToken}\n",
        "apiRoot = \"https://api.\" + tdpHostname\n",
        "SearchURL = apiRoot + \"/v1/datalake/searchEql\"\n",
        "\n",
        "SearchURL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G48F0BTJkwR"
      },
      "source": [
        "# Set Query to Search for All UNICORN System Names\n",
        "\n",
        "The TDP API uses ElasticSearch for indexing the UNICORN content. This powerful tool allows advanced searching against the data to find the appropriate information based upon your use case(s).\n",
        "\n",
        "In this scenario, we are creating a query to find all of the system names for the UNICORN data in the organization specified earlier.  The payload has three components:\n",
        "* Including `\"size\": 0` causes only the aggregation rusults to be returned, and not the search results themselves\n",
        "* The \"query\" section allows filtering, and we have 2 conditions:\n",
        "  * We only want IDS data in the \"akta\" schema, since there may be other IDS schemas with the \"data.system.name\" field\n",
        "  * We only want data from files which have the \"user\" label set during this workshop, requiring the \"nested\" query to check the label name and value\n",
        "* The \"aggs\" body includes a name for the aggregation so it can be referenced later in the notebook, the field to aggregate, and the maximum number of unique values to return.\n",
        "\n",
        "For more detailed information on aggregations using Elasticsearch: https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aadQZWN0M4uq"
      },
      "outputs": [],
      "source": [
        "payload = {\n",
        "  \"size\": 0,\n",
        "  \"query\": {\n",
        "    \"bool\": {\n",
        "      \"must\": [\n",
        "        {\n",
        "          \"term\": {\"idsType\": \"akta\"}\n",
        "        },\n",
        "        {\n",
        "          \"nested\": {\n",
        "            \"path\": \"labels\",\n",
        "            \"query\": {\n",
        "              \"bool\": {\n",
        "                \"must\": [\n",
        "                  {\"term\": {\"labels.name\": \"user\"}},\n",
        "                  {\"term\": {\"labels.value\": userLabel}}\n",
        "                ]\n",
        "              }\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  },\n",
        "  \"aggs\": {\n",
        "    \"unique_system_names\": {\n",
        "      \"terms\": { \"field\": \"data.system.name\", \"size\": 500 }\n",
        "    }\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9XsLGkgJ9aV"
      },
      "source": [
        "# Run Search Request and Display Results\n",
        "\n",
        "We will now use the requests library to make a request to the TDP API. We have previously configured the connection variables as well as the query we are executing.\n",
        "\n",
        "Our final line just has the variable \"result\" in it which tells the Notebook to print the value of the variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzavgKJLJ_H_"
      },
      "outputs": [],
      "source": [
        "request = requests.post(SearchURL, json=payload, headers=headers)\n",
        "\n",
        "result = request.json()\n",
        "\n",
        "result"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92uU3d1DKTw_"
      },
      "source": [
        "Now let's isolate the results by accessing data by attribute names and assign it to a Pandas DataFrame object for viewing the data as a table. The sample data only came from 2 systems, but there could be many more results depending on the number of instruments and Agents.\n",
        "\n",
        "We are also renaming the columns in the DataFrame to make them more human readable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "emNYM0uRKbcd"
      },
      "outputs": [],
      "source": [
        "systems = result.get(\"aggregations\").get(\"unique_system_names\").get(\"buckets\")\n",
        "\n",
        "df = pandas.DataFrame(systems)\n",
        "df = df.rename(columns={\"key\": 'System Name', \"doc_count\": \"No Results\"})\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kgycad-yHg61"
      },
      "source": [
        "# Get All Results for the Systems and Display in a Table\n",
        "\n",
        "First let's create the payload for the API Query. We are retrieving values from the first two rows of the previous data table to pass in to the query, which should be the only rows.\n",
        "\n",
        "The \"payload_sys\" has several parts:\n",
        "* The \"size\" specifies the number of results we desire. This is the sum of the number of results from earlier.\n",
        "* The \"query\" body has two main parts:\n",
        "  * The \"must\" array block is a logical AND so all conditions in it must be true. Like the aggregate query earlier, we only want results in the \"akta\" schema, and with the appropriate \"user\" label.\n",
        "  * The \"should\" array block is a logical OR, so one or more conditions (based on \"minimum_should_match\" set below) should be true to be. We use this to include results associated with either of the System Names in the DataFrame built from the previous request.\n",
        "\n",
        "The search request with this payload is sent, and the hits returned are normalized into two DataFrame objects, which are joined to create the full dataset:\n",
        "* \"df_peaks\" contains the nested \"peaks\" data from each IDS file, and several important non-nested fields in the \"meta\" block\n",
        "* \"df_columns\" contains the nested \"columns\" data from each IDS file\n",
        "\n",
        "The \"Fields\" variable is being used for specifying the data of interest.  Notice how we use the json_normalize function to allow us to specify data in nested JSON objects.  We are also renaming the JSON paths to field names that are more human readable in the DataFrame.\n",
        "\n",
        "The derived column \"Peak End\" is created by adding the values in the \"Peak Start\" and \"Peak Width\" columns. This is added to the DataFrame, and \"Peak Width\" is dropped from the DataFrame.\n",
        "\n",
        "You'll notice that we are using the pandas DataFrame \"fillna\" function.  This allows us to replace Null values with something of our choosing.\n",
        "\n",
        "Lastly, we will sort the results by the Result Name and Peak Number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_a_KD-KctAK"
      },
      "outputs": [],
      "source": [
        "payload_sys = {\n",
        "  \"size\": int(df[\"No Results\"].sum()),\n",
        "  \"query\": {\n",
        "    \"bool\": {\n",
        "      \"must\": [\n",
        "        {\n",
        "          \"term\": {\"idsType\": \"akta\"}\n",
        "        },\n",
        "        {\"nested\": {\n",
        "          \"path\": \"labels\",\n",
        "          \"query\": {\n",
        "            \"bool\": {\n",
        "              \"must\": [\n",
        "                {\"term\": {\"labels.name\": \"user\"}},\n",
        "                {\"term\": {\"labels.value\": userLabel}}\n",
        "              ]\n",
        "            }\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    ],\n",
        "    \"should\": [\n",
        "      {\"term\": {\"data.system.name\": df[\"System Name\"][0]}},\n",
        "      {\"term\": {\"data.system.name\": df[\"System Name\"][1]}}\n",
        "    ],\n",
        "    \"minimum_should_match\" : 1,\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "request_sys = requests.post(SearchURL, json=payload_sys, headers=headers)\n",
        "result_sys = request_sys.json()\n",
        "\n",
        "allHits = result_sys.get(\"hits\").get(\"hits\")\n",
        "\n",
        "df_peaks = pandas.json_normalize(\n",
        "  allHits,\n",
        "  [\"_source\", [\"data\", \"result\", \"peaks\"]],\n",
        "  record_prefix='_source.data.result.peaks.',\n",
        "  meta=[[\"_source\", \"fileId\"]\n",
        "        , [\"_source\", \"data\", \"system\", \"name\"]\n",
        "        , [\"_source\", \"data\", \"method\", \"instrument\", \"name\"]\n",
        "        , [\"_source\", \"data\", \"result\", \"name\"]\n",
        "        , [\"_source\", \"data\", \"result\", \"created_at\"]])\n",
        "\n",
        "df_columns = pandas.json_normalize(\n",
        "  allHits,\n",
        "  [\"_source\", [\"data\", \"method\", \"instrument\", \"columns\"]],\n",
        "  record_prefix='_source.data.method.instrument.columns.')\n",
        "\n",
        "df_all = df_peaks.join(df_columns)\n",
        "\n",
        "Fields = [\"_source.fileId\"\n",
        "          , \"_source.data.system.name\"\n",
        "          , \"_source.data.method.instrument.name\"\n",
        "          , \"_source.data.method.instrument.columns.name\"\n",
        "          , \"_source.data.result.name\"\n",
        "          , \"_source.data.result.created_at\"\n",
        "          , \"_source.data.result.peaks.number\"\n",
        "          , \"_source.data.result.peaks.name\"\n",
        "          , \"_source.data.result.peaks.area.value\"\n",
        "          , \"_source.data.result.peaks.area.unit\"\n",
        "          , \"_source.data.result.peaks.retention_volume.value\"\n",
        "          , \"_source.data.result.peaks.retention_volume.unit\"\n",
        "          , \"_source.data.result.peaks.start.value\"\n",
        "          , \"_source.data.result.peaks.start.unit\"\n",
        "          , \"_source.data.result.peaks.width.value\"]\n",
        "\n",
        "renamedDataFrame = df_all[Fields].rename(\n",
        "    columns={\"_source.fileId\": \"File Id\"\n",
        "             , \"_source.data.system.name\": \"System Name\"\n",
        "             , \"_source.data.method.instrument.name\": \"Instrument Name\"\n",
        "             , \"_source.data.method.instrument.columns.name\": \"Column Name\"\n",
        "             , \"_source.data.result.name\": \"Result Name\"\n",
        "             , \"_source.data.result.created_at\": \"File Creation Time\"\n",
        "             , \"_source.data.result.peaks.number\": \"Peak Number\"\n",
        "             , \"_source.data.result.peaks.name\": \"Peak Name\"\n",
        "             , \"_source.data.result.peaks.area.value\": \"Peak Area Value\"\n",
        "             , \"_source.data.result.peaks.area.unit\": \"Peak Area Unit\"\n",
        "             , \"_source.data.result.peaks.retention_volume.value\": \"Retention Volume Value\"\n",
        "             , \"_source.data.result.peaks.retention_volume.unit\": \"Retention Volume Unit\"\n",
        "             , \"_source.data.result.peaks.start.value\": \"Peak Start Value\"\n",
        "             , \"_source.data.result.peaks.start.unit\": \"Peak Start Unit\"\n",
        "             , \"_source.data.result.peaks.width.value\": \"Peak Width Value\"})\n",
        "\n",
        "renamedDataFrame[\"Peak End Value\"] = renamedDataFrame[\"Peak Start Value\"] + renamedDataFrame[\"Peak Width Value\"]\n",
        "renamedDataFrame.drop(\"Peak Width Value\", axis=1, inplace=True)\n",
        "\n",
        "renamedDataFrame[\"Column Name\"].fillna(\"Missing Value\", inplace=True)\n",
        "renamedDataFrame.sort_values(by=[\"Result Name\", \"Peak Number\"], inplace=True)\n",
        "renamedDataFrame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY4JDpjJvp-A"
      },
      "source": [
        "# Display a Chart Showing the Peak Area\n",
        "\n",
        "Let's compare the results from the 3 runs with similar names starting with \"190213 5mL ProA EluteDesalt with Buffer change\" by graphing the Peak Area of those results. We will use a scatter plot to easily tell if the data points are identical (fully overlapping) or different (not touching or with a partial overlap).\n",
        "\n",
        "We know there will be 3 runs in the data set, so the `markers` array is defined explicitly with 3 different shapes. The marker styles are documented here: https://matplotlib.org/stable/api/markers_api.html\n",
        "\n",
        "`mCount` is used to ensure that if there is unexpectedly more data, the marker shapes will be reused instead of throwing an exception.\n",
        "\n",
        "Then, we create a new `comparison_df` DataFrame by selecting only those rows which have a \"Result Name\" starting with the value seen in the results table \"Result Name\" column, without the ellipsis (`...`) at the end.\n",
        "\n",
        "The units for the Retention Volume and Peak Area should be consistent across all rows of the DataFrame, so the first (and only) unique value is selected and saved for reference later.\n",
        "\n",
        "Next, the results are grouped by the Result Name so each run will have its data plotted as a series. For each group, the Peak Area is plotted, and shown with a different color (assigned automatically) and shape (based on the chosen markers).\n",
        "\n",
        "Due to the length of the result name, the legend is set outside of the chart, and the labels are set, including the units for each axis based on the variables populated from the DataFrame."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u-6u32sZ2rI7"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots();\n",
        "\n",
        "markers = [\"o\", \"^\", \"P\"]\n",
        "mCount = 0\n",
        "\n",
        "comparison_df = renamedDataFrame[renamedDataFrame[\"Result Name\"]\n",
        "                                 .str.startswith(\"190213 5mL ProA EluteDesalt with Buffer change\")]\n",
        "\n",
        "retentionVolUnit = comparison_df[\"Retention Volume Unit\"].unique()[0]\n",
        "peakAreaUnit = comparison_df[\"Peak Area Unit\"].unique()[0]\n",
        "\n",
        "for name, grp in comparison_df.groupby(\"Result Name\"):\n",
        "    ax.plot(grp[\"Retention Volume Value\"], grp[\"Peak Area Value\"], marker=markers[mCount], linestyle=\"\", ms=12, label=name)\n",
        "    mCount += 1\n",
        "    if mCount == len(markers) :\n",
        "      # Reuse markers if more data is plotted\n",
        "      mCount = 0\n",
        "\n",
        "ax.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "ax.set_xlabel(f\"Retention Volume ({retentionVolUnit})\")\n",
        "ax.set_ylabel(f\"Peak Area ({peakAreaUnit})\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoONZEaS9Uw3"
      },
      "source": [
        "Note that 2 of the runs (ending in \"005\" and \"006\") have the same peak area and retention volume values, so their plot points overlap exactly. Depending on the colors and shape of the markers, they may not all be visible in the chart.\n",
        "\n",
        "But the other run (ending in \"003\") has slightly lower values for Peak 1 and Peak 3. You can check the exact values in the original table to determine how much the difference is for each peak, and possibly continue analyzing the data to identify why this occurred."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
