{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0b64df",
   "metadata": {},
   "source": [
    "# Data Synchronization\n",
    "\n",
    "Sometimes you would like to “synchronize” data from TDP to other downstream systems, to bring data to existing analytical systems for your users (SDEs, Data Scientists, Data Analysts) are accustomed to working with.  This could be:\n",
    "\n",
    "* Any new or updated data that’s ingested into TDP should be available in the downstream system\n",
    "* A particular subset of data (but is generally “all data” or “all data from xyz inputs”)\n",
    "\n",
    "Examples:\n",
    "\n",
    "* Bring data from JSONs created by Tetra Data IDS into Snowflake or another OLAP database in order to do more performant exploratory data analysis / build transformed & aggregated SQL tables\n",
    "* Copy data into a specific software\n",
    "\n",
    "In v4.0+ of Tetra Data Platform, the capability also exists to “listen” or “subscribe” to data updates via [Event Subscriptions](https://developers.tetrascience.com/docs/event-subscriptions) . However, should your use case require a polling mechanism, the following implementation can provide a performant and scalable way to do this.\n",
    "\n",
    "Note: This solution pattern does not cover changes in downstream systems being propagated back into Tetra Data Platform "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496a433",
   "metadata": {},
   "source": [
    "## Solution Overview\n",
    "\n",
    "1. Set up a regular process to hit the searchEQL API to pull updated data since last “poll” with a query that is for your specific subset\n",
    "    a. “Poll” Search API for updated data based on date / time\n",
    "2. For each result in the query, retrieve the data with the /v1/datalake/retrieve endpoint\n",
    "3. Process your data into your specific data target (will be a specific solution to your target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52df0ba6",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fa1468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913e78bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTH_FILE_DIR = \"./\"\n",
    "AUTH_FILENAME = \"auth.json\"\n",
    "API_SEARCHEQL = \"https://api.tetrascience.com/v1/datalake/searchEql\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55606888",
   "metadata": {},
   "source": [
    "## 1. “Poll” Search API for updated data based on date / time\n",
    "\n",
    "In order to synchronize data effectively, use a process that makes calls to the /v1/datalake/searchEql API endpoint on a regular cadence (e.g. every 4 hours) that pulls data which has been updated since then and then do an “upsert” or “merge” into the downstream system using the indexedAt field to query based on a particular date\n",
    "\n",
    "All data from this search will have an indexedAt value that represents when the data was last updated.\n",
    "\n",
    "If results of the query are expected to exceed 10,000 results, the solution will need to implement support for pagination.\n",
    "\n",
    "The consuming program will need to maintain a mechanism to ensure that no data is “skipped” by looking back at date / times and processing for duplicates (which upsert / merge can handle) or keeping track of the “last date / time” polled. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f04a26f",
   "metadata": {},
   "source": [
    "### Use authenication file for API headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a952c5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(AUTH_FILE_DIR, AUTH_FILENAME), \"r\") as f:\n",
    "    auth_data = json.loads(f.read())\n",
    "\n",
    "headers = {\"ts-auth-token\": auth_data[\"auth_token\"],\n",
    "           \"x-org-slug\": auth_data[\"org\"],\n",
    "           \"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9a99c8",
   "metadata": {},
   "source": [
    "### Example query where IDS data contains project information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5ad560",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_basic = {\n",
    "                  \"query\": {\n",
    "                      \"match\": {\n",
    "                          \"data.project.name\": \"my_project\"\n",
    "                      }\n",
    "                  }\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0217837",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_searcheql_basic = requests.post(API_SEARCHEQL, headers=headers, data=json.dumps(query_basic))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0196acf6",
   "metadata": {},
   "source": [
    "### Example query with date range\n",
    "\n",
    "* \"size\": Can be anything reasonable up to 10,000 and defines how many results we will get per query. A good upper bound for general use is 1,000.\n",
    "\n",
    "* \"query\": The query itself, built out of the components\n",
    "\n",
    "* \"bool\" (boolean query): To search for files that match boolean combinations of a set of queries.\n",
    "\n",
    "* \"must\":  The clause (query) must appear in matching files. This is like logical AND. \n",
    "\n",
    "* \"term\": The exact term that's found in files, in this case the IDS JSON. For example, in this query, the query returns files that have the category \"IDS\" and the source.type \"empower\". Other fields can be added if needed.\n",
    "\n",
    "* \"range\": This defines the dates over which to query the indexedAt field. The greater-than-or-equals time should always be the same as the less-than time of the previous execution of the job. The times are specified here in UTC. The query format supports time-zone information should it be necessary. The less-than time must always be in the past to avoid missing any files.\n",
    "\n",
    "* \"_source\": This defines what fields you want returned from your query. Here only the fileId is returned. You can speed up query times by specifying only a few fields.\n",
    "\n",
    "* \"sort\": This specifies how you would like the returned search results to be ordered. Here it is sorted in an ascending manner by the fileId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7f0cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_empower = {\n",
    "                   \"size\": 50,\n",
    "                   \"query\": {\n",
    "                       \"bool\": {\n",
    "                           \"must\": [\n",
    "                               {\n",
    "                                   \"term\": { \"source.type\": \"empower\" }\n",
    "                               },\n",
    "                               {\n",
    "                                   \"term\": { \"category\": \"IDS\" }\n",
    "                               },\n",
    "                               {\n",
    "                                   \"range\": {\n",
    "                                       \"indexedAt\": {\n",
    "                                          \"gte\": \"2023-03-01T00:00:00\",\n",
    "                                          \"lt\": \"2023-03-20T00:00:00\"\n",
    "                                       }\n",
    "                                   }\n",
    "                               }\n",
    "                           ]\n",
    "                       }\n",
    "                   },\n",
    "                   \"_source\": [ \"fileId\" ],\n",
    "                   \"sort\": [\n",
    "                       {\"fileId\": \"asc\"}\n",
    "                   ]\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d55841",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_searcheql_empower = requests.post(API_SEARCHEQL, headers=headers, data=json.dumps(query_empower))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a7efe8",
   "metadata": {},
   "source": [
    "### Example subsequent query\n",
    "\n",
    "If you want to search the same set of files but return the files after the last query, add the \"search_after\" criteria of the fileId of the final result of the previous data set. Then it returns the next 50 items (as specified in the query) after the last search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ab8f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_empower_next = {\n",
    "                        \"size\": 50,\n",
    "                        \"query\": {\n",
    "                            \"bool\": {\n",
    "                                \"must\": {\n",
    "                                    \"term\": { \"source.type\": \"empower\" }\n",
    "                                },\n",
    "                                \"must\": {\n",
    "                                    \"term\": { \"category\": \"IDS\" }\n",
    "                                },\n",
    "                                {\n",
    "                                    \"range\": {\n",
    "                                        \"indexedAt\": {\n",
    "                                            \"gte\": \"2023-03-01T00:00:00\",\n",
    "                                            \"lt\": \"2023-03-20T00:00:00\"\n",
    "                                         }\n",
    "                                     }\n",
    "                                 }\n",
    "                            }\n",
    "                        },\n",
    "                        \"_source\": [ \"fileId\" ],\n",
    "                        \"search_after\": [\"11ee50ef-6b0d-4266-863d-461d02f6f1b1\"],\n",
    "                        \"sort\": [\n",
    "                            {\"fileId\": \"asc\"}\n",
    "                        ]\n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175b5350",
   "metadata": {},
   "source": [
    "### Example query returned items\n",
    "\n",
    "This Empower API calls above (and other queries you construct) will return a response that contains objects representing “files” to process.  You will want to iterate over each record in the hits.hits[] array."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08850aa2",
   "metadata": {},
   "source": [
    "## 2. Retrieve the data with the /v1/datalake/retrieve endpoint\n",
    "\n",
    "Using the object in the hits.hits[] array, use the source.source.fileId to retrieve the actual data from the https://api.tetrascience.com/v1/datalake/retrieve API endpoint.\n",
    "\n",
    "See documentation: https://developers.tetrascience.com/reference/retrieve-a-file \n",
    "\n",
    "This will return either a:\n",
    "* an application/octet-stream file contain the data record\n",
    "* a JSON object with a S3 URL to download (if preSigned parameter was set to true in API request)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941b3513",
   "metadata": {},
   "source": [
    "## 3. Access the file and then process / transform into the appropriate tables / schema that you desire."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
